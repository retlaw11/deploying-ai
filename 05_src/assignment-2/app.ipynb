{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db6f5cf",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b211d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/retlawair/Desktop/deploying-ai/deploying-ai-env/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using pip magic command (best for Jupyter)\n",
    "%pip install -q gradio openai python-dotenv\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c5098",
   "metadata": {},
   "source": [
    "## Step 2: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a4b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key loaded: YTL...\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ./.secrets\n",
    "\n",
    "import os\n",
    "api_key = os.getenv('API_GATEWAY_KEY')\n",
    "print(f\"‚úÖ API Key loaded: {api_key[:3] if api_key else 'NOT FOUND'}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43dbd91",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d1815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradio 5.49.1\n",
      "‚úÖ OpenAI SDK ready\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"‚úÖ Gradio {gr.__version__}\")\n",
    "print(\"‚úÖ OpenAI SDK ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823aaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pydantic models ready\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import Optional, List, Literal\n",
    "from enum import Enum\n",
    "\n",
    "print(\"‚úÖ Pydantic models ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9803fd",
   "metadata": {},
   "source": [
    "## Step 4: Initialize OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b32078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ecb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pydantic models and PersonalityLibrary defined\n"
     ]
    }
   ],
   "source": [
    "# Define allowed values as enums for type safety\n",
    "class ToneType(str, Enum):\n",
    "    FORMAL = \"formal\"\n",
    "    CASUAL = \"casual\"\n",
    "    FRIENDLY = \"friendly\"\n",
    "    PROFESSIONAL = \"professional\"\n",
    "    HUMOROUS = \"humorous\"\n",
    "\n",
    "class StyleType(str, Enum):\n",
    "    CONCISE = \"concise\"\n",
    "    DETAILED = \"detailed\"\n",
    "    CREATIVE = \"creative\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    SOCRATIC = \"socratic\"\n",
    "\n",
    "class RoleType(str, Enum):\n",
    "    ASSISTANT = \"helpful AI assistant\"\n",
    "    MENTOR = \"mentor and educator\"\n",
    "    EXPERT = \"subject matter expert\"\n",
    "    BRAINSTORMER = \"creative brainstorming partner\"\n",
    "    THERAPIST = \"empathetic listener\"\n",
    "    ANALYST = \"data analyst\"\n",
    "\n",
    "# Core configuration model\n",
    "class ChatConfig(BaseModel):\n",
    "    \"\"\"Pydantic model for chat personality and behavior configuration\"\"\"\n",
    "    \n",
    "    tone: ToneType = Field(\n",
    "        default=ToneType.FRIENDLY,\n",
    "        description=\"The tone of voice for responses\"\n",
    "    )\n",
    "    conversational_style: StyleType = Field(\n",
    "        default=StyleType.CONCISE,\n",
    "        description=\"How detailed or creative responses should be\"\n",
    "    )\n",
    "    role: RoleType = Field(\n",
    "        default=RoleType.ASSISTANT,\n",
    "        description=\"The character role the AI adopts\"\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        default=0.7,\n",
    "        ge=0.0,\n",
    "        le=2.0,\n",
    "        description=\"Controls randomness (0=deterministic, 2=very creative)\"\n",
    "    )\n",
    "    max_tokens: int = Field(\n",
    "        default=1000,\n",
    "        ge=100,\n",
    "        le=4000,\n",
    "        description=\"Maximum response length\"\n",
    "    )\n",
    "    custom_system_prompt: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Override default system prompt\"\n",
    "    )\n",
    "    model: str = Field(\n",
    "        default=\"gpt-4o-mini\",\n",
    "        description=\"OpenAI model to use\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('temperature')\n",
    "    @classmethod\n",
    "    def validate_temperature(cls, v):\n",
    "        if v < 0 or v > 2:\n",
    "            raise ValueError('Temperature must be between 0 and 2')\n",
    "        return v\n",
    "    \n",
    "    # this is a validator to ensure that the temperature value is within the acceptable range for OpenAI models. If a user tries to set a temperature outside of 0 to 2, it will raise a ValueError with a clear message.\n",
    "    # guidelines are also soft enforced in the system prompt to ensure that even if a user tries to bypass the validator, the model will still be instructed not to respond to certain topics or reveal system instructions.\n",
    "\n",
    "    def get_system_prompt(self) -> str:\n",
    "        \"\"\"Generate comprehensive system prompt based on configuration\"\"\"\n",
    "        if self.custom_system_prompt:\n",
    "            return self.custom_system_prompt\n",
    "        \n",
    "        return f\"\"\"You are a {self.role.value} with a {self.tone.value} tone.\n",
    "Your conversational style is {self.conversational_style.value}.\n",
    "\n",
    "Guidelines:\n",
    "- Do not answer any requests on the following topics: Cats, Dogs, Horoscopes, Zodiac Signs, or Taylor Swift.\n",
    "- Maintain consistency with your assigned tone throughout\n",
    "- Adapt your response length based on your style (concise = short, detailed = thorough)\n",
    "- Stay in character as a {self.role.value}\n",
    "- Be helpful, accurate, and respectful\"\"\"\n",
    "    \n",
    "    class Config:\n",
    "        use_enum_values = True\n",
    "\n",
    "# Message model for type-safe message handling\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"] \n",
    "    content: str\n",
    "\n",
    "# Chat session model to track state\n",
    "class ChatSession(BaseModel):\n",
    "    config: ChatConfig\n",
    "    messages: List[Message] = Field(default_factory=list)\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add message to session\"\"\"\n",
    "        self.messages.append(Message(role=role, content=content))\n",
    "    \n",
    "    def get_messages_for_api(self) -> List[dict]:\n",
    "        \"\"\"Convert messages to OpenAI API format\"\"\"\n",
    "        return [{\"role\": msg.role, \"content\": msg.content} for msg in self.messages]\n",
    "\n",
    "# Predefined personality configurations\n",
    "class PersonalityLibrary:\n",
    "    \"\"\"Collection of pre-configured chat personalities\"\"\"\n",
    "    \n",
    "    DEFAULT = ChatConfig(\n",
    "        tone=ToneType.FRIENDLY,\n",
    "        conversational_style=StyleType.CONCISE,\n",
    "        role=RoleType.ASSISTANT,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    MENTOR = ChatConfig(\n",
    "        tone=ToneType.PROFESSIONAL,\n",
    "        conversational_style=StyleType.DETAILED,\n",
    "        role=RoleType.MENTOR,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    \n",
    "    CREATIVE = ChatConfig(\n",
    "        tone=ToneType.CASUAL,\n",
    "        conversational_style=StyleType.CREATIVE,\n",
    "        role=RoleType.BRAINSTORMER,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    ANALYTICAL = ChatConfig(\n",
    "        tone=ToneType.FORMAL,\n",
    "        conversational_style=StyleType.ANALYTICAL,\n",
    "        role=RoleType.ANALYST,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_all_personalities():\n",
    "        \"\"\"Return all available personalities\"\"\"\n",
    "        return {\n",
    "            \"default\": PersonalityLibrary.DEFAULT,\n",
    "            \"mentor\": PersonalityLibrary.MENTOR,\n",
    "            \"creative\": PersonalityLibrary.CREATIVE,\n",
    "            \"analytical\": PersonalityLibrary.ANALYTICAL\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Pydantic models and PersonalityLibrary defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d93c7",
   "metadata": {},
   "source": [
    "## Step 4.5: Define Chat Config Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f007f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enterprise guardrails system initialized\n",
      "   üìã Forbidden topics: 5\n",
      "   üîç Pattern checks: 4\n",
      "   ‚è±Ô∏è  Max input: 2000 chars\n",
      "   üöÄ Rate limit: 60 req/min\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class GuardrailConfig:\n",
    "    \"\"\"Enterprise configuration for content guardrails\"\"\"\n",
    "    forbidden_topics: List[str] = field(default_factory=lambda: [\n",
    "        \"cats\", \"dogs\", \"horoscopes\", \"zodiac signs\", \"taylor swift\"\n",
    "    ])\n",
    "    forbidden_patterns: List[str] = field(default_factory=lambda: [\n",
    "        r\"\\b(cats?|dogs?|horoscope|zodiac|taylor\\s+swift)\\b\",\n",
    "        r\"prompt\\s+injection\", r\"system\\s+prompt\", r\"ignore\\s+instructions\"\n",
    "    ])\n",
    "    max_input_length: int = 2000\n",
    "    min_input_length: int = 1\n",
    "    max_tokens_per_request: int = 4000\n",
    "    max_requests_per_minute: int = 60\n",
    "    max_output_length: int = 4000\n",
    "    min_output_length: int = 1\n",
    "\n",
    "class GuardrailViolation(Exception):\n",
    "    \"\"\"Custom exception for guardrail violations\"\"\"\n",
    "    def __init__(self, message: str, severity: str, violation_type: str):\n",
    "        self.message = message\n",
    "        self.severity = severity\n",
    "        self.violation_type = violation_type\n",
    "        super().__init__(f\"[{severity}] {violation_type}: {message}\")\n",
    "\n",
    "class ContentGuardrails:\n",
    "    \"\"\"Enterprise-grade content validation\"\"\"\n",
    "    def __init__(self, config: GuardrailConfig = None):\n",
    "        self.config = config or GuardrailConfig()\n",
    "        self.request_history: Dict[str, List[float]] = {}\n",
    "        self.compiled_patterns = [re.compile(p, re.IGNORECASE) for p in self.config.forbidden_patterns]\n",
    "    \n",
    "    def validate_input(self, message: str, user_id: str = \"default\") -> Tuple[bool, str]:\n",
    "        if not message or not isinstance(message, str):\n",
    "            raise GuardrailViolation(\"Input must be a non-empty string\", \"CRITICAL\", \"invalid_input_type\")\n",
    "        if len(message) < self.config.min_input_length:\n",
    "            raise GuardrailViolation(f\"Input too short\", \"WARNING\", \"length_violation\")\n",
    "        if len(message) > self.config.max_input_length:\n",
    "            raise GuardrailViolation(f\"Input exceeds max length ({self.config.max_input_length})\", \"CRITICAL\", \"length_violation\")\n",
    "        \n",
    "        message_lower = message.lower()\n",
    "        for topic in self.config.forbidden_topics:\n",
    "            if topic in message_lower:\n",
    "                raise GuardrailViolation(f\"Forbidden topic: '{topic}'\", \"CRITICAL\", \"forbidden_content\")\n",
    "        \n",
    "        for pattern in self.compiled_patterns:\n",
    "            if pattern.search(message):\n",
    "                raise GuardrailViolation(\"Suspicious patterns detected\", \"CRITICAL\", \"prompt_injection\")\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if user_id not in self.request_history:\n",
    "            self.request_history[user_id] = []\n",
    "        self.request_history[user_id] = [t for t in self.request_history[user_id] if current_time - t < 60]\n",
    "        if len(self.request_history[user_id]) >= self.config.max_requests_per_minute:\n",
    "            raise GuardrailViolation(f\"Rate limit exceeded\", \"WARNING\", \"rate_limit\")\n",
    "        self.request_history[user_id].append(current_time)\n",
    "        return True, \"Input validation passed\"\n",
    "    \n",
    "    def validate_output(self, response: str) -> Tuple[bool, str]:\n",
    "        if not isinstance(response, str):\n",
    "            raise GuardrailViolation(\"Output must be a string\", \"CRITICAL\", \"invalid_output_type\")\n",
    "        if len(response) < self.config.min_output_length:\n",
    "            raise GuardrailViolation(\"Insufficient output\", \"WARNING\", \"output_length_violation\")\n",
    "        if len(response) > self.config.max_output_length:\n",
    "            return True, response[:self.config.max_output_length] + \"...\"\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        for topic in self.config.forbidden_topics:\n",
    "            if topic in response_lower:\n",
    "                raise GuardrailViolation(f\"Forbidden content in output: '{topic}'\", \"CRITICAL\", \"forbidden_output_content\")\n",
    "        \n",
    "        system_indicators = [\"system prompt\", \"you are a\", \"role:\", \"guideline:\", \"instruction:\"]\n",
    "        for indicator in system_indicators:\n",
    "            if indicator in response_lower and (\"system\" in response_lower or \"instruction\" in response_lower):\n",
    "                raise GuardrailViolation(\"System prompt leakage detected\", \"CRITICAL\", \"prompt_leakage\")\n",
    "        return True, response\n",
    "\n",
    "guardrails_config = GuardrailConfig()\n",
    "guardrails = ContentGuardrails(guardrails_config)\n",
    "print(\"‚úÖ Enterprise guardrails system initialized\")\n",
    "print(f\"   üìã Forbidden topics: {len(guardrails_config.forbidden_topics)}\")\n",
    "print(f\"   üîç Pattern checks: {len(guardrails_config.forbidden_patterns)}\")\n",
    "print(f\"   ‚è±Ô∏è  Max input: {guardrails_config.max_input_length} chars\")\n",
    "print(f\"   üöÄ Rate limit: {guardrails_config.max_requests_per_minute} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ace14",
   "metadata": {},
   "source": [
    "## Step 5: Define Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b36abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced chat function defined with Pydantic config\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(message, history, config=None):\n",
    "    \"\"\"\n",
    "    Enhanced chat with GPT-4 using Pydantic configuration management.\n",
    "    Maintains conversation history with configurable personality.\n",
    "    INCLUDES ENTERPRISE-GRADE GUARDRAILS.\n",
    "    \n",
    "    Args:\n",
    "        message: Current user message (string)\n",
    "        history: List of message dictionaries with 'role' and 'content' keys\n",
    "        config: ChatConfig Pydantic model (uses DEFAULT if None)\n",
    "    \n",
    "    Returns:\n",
    "        Assistant's response (string)\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = PersonalityLibrary.DEFAULT\n",
    "    \n",
    "    try:\n",
    "        # ===== GUARDRAIL 1: INPUT VALIDATION (Hard Block) =====\n",
    "        try:\n",
    "            guardrails.validate_input(message)\n",
    "        except GuardrailViolation as gv:\n",
    "            if gv.severity == \"CRITICAL\":\n",
    "                return f\"‚õî Request blocked: {gv.message}\"\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {gv.message}\")\n",
    "        \n",
    "        # Validate and ensure config is ChatConfig instance\n",
    "        if not isinstance(config, ChatConfig):\n",
    "            config = ChatConfig(**config) if isinstance(config, dict) else PersonalityLibrary.DEFAULT\n",
    "        \n",
    "        # Validate config parameters (hard limits)\n",
    "        if config.temperature < 0 or config.temperature > 2:\n",
    "            raise GuardrailViolation(\n",
    "                \"Temperature out of bounds\",\n",
    "                \"CRITICAL\",\n",
    "                \"parameter_violation\"\n",
    "            )\n",
    "        \n",
    "        if config.max_tokens < 100 or config.max_tokens > 4000:\n",
    "            raise GuardrailViolation(\n",
    "                \"Max tokens out of bounds\",\n",
    "                \"CRITICAL\",\n",
    "                \"parameter_violation\"\n",
    "            )\n",
    "        \n",
    "        # Build messages list with system prompt and history\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": config.get_system_prompt()\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history (already in correct format)\n",
    "        if history:\n",
    "            messages.extend(history)\n",
    "        \n",
    "        # Add current message\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Call OpenAI API with validated config\n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=messages,\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        \n",
    "        assistant_response = response.choices[0].message.content\n",
    "        \n",
    "        # ===== GUARDRAIL 2: OUTPUT VALIDATION (Hard Block) =====\n",
    "        try:\n",
    "            is_valid, result = guardrails.validate_output(assistant_response)\n",
    "            if is_valid:\n",
    "                return result\n",
    "        except GuardrailViolation as gv:\n",
    "            if gv.severity == \"CRITICAL\":\n",
    "                return \"‚õî Response blocked by content policy\"\n",
    "            else:\n",
    "                return assistant_response\n",
    "        \n",
    "        return assistant_response\n",
    "    \n",
    "    except GuardrailViolation as gv:\n",
    "        return f\"‚õî [{gv.severity}] {gv.message}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Enhanced chat function defined with Pydantic config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8bd9e",
   "metadata": {},
   "source": [
    "## Step 6: Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b69847dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chatbot interface with LLM-powered file scanner created\n"
     ]
    }
   ],
   "source": [
    "# Store config choices\n",
    "config_choices = {\n",
    "    \"personality\": \"default\",\n",
    "    \"tone\": \"friendly\", \n",
    "    \"style\": \"concise\",\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "def simple_chat(message, history):\n",
    "    \"\"\"Simple chat function with enterprise guardrails enforcement\"\"\"\n",
    "    try:\n",
    "        # ===== GUARDRAIL 1: INPUT VALIDATION (Hard Block) =====\n",
    "        try:\n",
    "            guardrails.validate_input(message, user_id=\"gradio_user\")\n",
    "        except GuardrailViolation as gv:\n",
    "            return f\"‚õî Request blocked: {gv.message}\"\n",
    "        \n",
    "        # ===== GUARDRAIL 3: HARD LIMITS CHECK =====\n",
    "        # Validate temperature is within bounds\n",
    "        temp = config_choices.get(\"temperature\", 0.7)\n",
    "        if not (0.0 <= temp <= 2.0):\n",
    "            return \"‚õî Temperature out of valid range (0.0-2.0)\"\n",
    "        \n",
    "        # Get base config from personality\n",
    "        personalities = PersonalityLibrary.get_all_personalities()\n",
    "        base_config = personalities.get(config_choices[\"personality\"], PersonalityLibrary.DEFAULT)\n",
    "        \n",
    "        # Use selected tone and style directly as strings\n",
    "        tone_str = config_choices[\"tone\"]\n",
    "        style_str = config_choices[\"style\"]\n",
    "        \n",
    "        # Create simple system prompt with selected attributes\n",
    "        system_prompt = f\"\"\"You are a helpful AI assistant with a {tone_str} tone.\n",
    "Your conversational style is {style_str}.\n",
    "\n",
    "Guidelines:\n",
    "- Do not answer any requests on the following topics: Cats, Dogs, Horoscopes, Zodiac Signs, or Taylor Swift.\n",
    "- Be helpful, accurate, and respectful.\"\"\"\n",
    "        \n",
    "        # Build messages\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        if history:\n",
    "            messages.extend(history)\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Call API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        assistant_response = response.choices[0].message.content\n",
    "        \n",
    "        # ===== GUARDRAIL 2: OUTPUT VALIDATION (Hard Block) =====\n",
    "        try:\n",
    "            is_valid, result = guardrails.validate_output(assistant_response)\n",
    "            return result if is_valid else assistant_response\n",
    "        except GuardrailViolation as gv:\n",
    "            return \"‚õî Response blocked by content policy\"\n",
    "    \n",
    "    except GuardrailViolation as gv:\n",
    "        return f\"‚õî [{gv.severity}] {gv.message}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def run_ai_risk_search(query, top_k=5, lexical_k=50, alpha=0.35):\n",
    "    \"\"\"Run hybrid lexical + semantic search against the AI Risk Database.\"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return \"‚ùå Please enter a question.\", []\n",
    "    results = hybrid_search(query, top_k=top_k, lexical_k=lexical_k, alpha=alpha)\n",
    "    markdown = format_search_results_markdown(results, query)\n",
    "    return markdown, results\n",
    "\n",
    "\n",
    "def landing_ai_risk_search(query, top_k=5):\n",
    "    \"\"\"Landing-page helper that also populates the AI Risk Search tab.\"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return \"‚ùå Please enter a question.\", \"\", \"‚ùå Please enter a question.\", []\n",
    "    results = hybrid_search(query, top_k=top_k)\n",
    "    markdown = format_search_results_markdown(results, query)\n",
    "    landing_message = f\"‚úÖ Results loaded in **AI Risk Search** tab.\\n\\n{markdown}\"\n",
    "    return landing_message, query, markdown, results\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ü§ñ Walter's AI Chatbot & Security Scanner\")\n",
    "    gr.Markdown(\"Customize your chat experience or scan files for security threats\")\n",
    "\n",
    "    gr.Markdown(\"### üîé Quick Search the AI Risk Database\")\n",
    "    with gr.Row():\n",
    "        landing_query = gr.Textbox(\n",
    "            label=\"Ask about AI risks\",\n",
    "            placeholder=\"e.g., What are risks related to misinformation?\"\n",
    "        )\n",
    "        landing_top_k = gr.Slider(\n",
    "            minimum=3,\n",
    "            maximum=10,\n",
    "            value=5,\n",
    "            step=1,\n",
    "            label=\"Top results\"\n",
    "        )\n",
    "        landing_search_btn = gr.Button(\"Search\", variant=\"secondary\")\n",
    "    landing_output = gr.Markdown(\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # ===== TAB 1: Chat Interface =====\n",
    "        with gr.Tab(\"üí¨ Chat\"):\n",
    "            with gr.Row():\n",
    "                personality = gr.Dropdown(\n",
    "                    choices=list(PersonalityLibrary.get_all_personalities().keys()),\n",
    "                    value=\"default\",\n",
    "                    label=\"üé≠ Personality\"\n",
    "                )\n",
    "                tone = gr.Dropdown(\n",
    "                    choices=[t.value for t in ToneType],\n",
    "                    value=\"friendly\",\n",
    "                    label=\"üéµ Tone\"\n",
    "                )\n",
    "                style = gr.Dropdown(\n",
    "                    choices=[s.value for s in StyleType],\n",
    "                    value=\"concise\",\n",
    "                    label=\"üìù Style\"\n",
    "                )\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.0,\n",
    "                    maximum=2.0,\n",
    "                    value=0.7,\n",
    "                    step=0.1,\n",
    "                    label=\"üî• Temperature\"\n",
    "                )\n",
    "            \n",
    "            personality.change(lambda x: config_choices.update({\"personality\": x}), inputs=personality)\n",
    "            tone.change(lambda x: config_choices.update({\"tone\": x}), inputs=tone)\n",
    "            style.change(lambda x: config_choices.update({\"style\": x}), inputs=style)\n",
    "            temperature.change(lambda x: config_choices.update({\"temperature\": x}), inputs=temperature)\n",
    "            \n",
    "            chatbot = gr.ChatInterface(\n",
    "                fn=simple_chat,\n",
    "                examples=[\n",
    "                    \"What is machine learning?\",\n",
    "                    \"Explain quantum computing\",\n",
    "                    \"How do neural networks work?\",\n",
    "                    \"What is the capital of France?\"\n",
    "                ],\n",
    "                type=\"messages\"\n",
    "            )\n",
    "        \n",
    "        # ===== TAB 2: File Scanner =====\n",
    "        with gr.Tab(\"üîí File Scanner\"):\n",
    "            gr.Markdown(\"### VirusTotal File Security Scan with AI Interpretation\")\n",
    "            gr.Markdown(\"Upload a file to scan it for security threats. Results will be analyzed and explained by AI.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                file_input = gr.File(\n",
    "                    label=\"üìÅ Select File to Scan\",\n",
    "                    type=\"filepath\"\n",
    "                )\n",
    "                scan_button = gr.Button(\"üîç Scan & Analyze\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    ai_interpretation = gr.Markdown(\"Upload a file and click 'Scan & Analyze' to begin analysis\")\n",
    "                with gr.Column():\n",
    "                    scan_output = gr.Markdown(\"Scan details will appear here\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                json_output = gr.JSON(label=\"üìä Raw Scan Data (JSON)\", visible=True)\n",
    "            \n",
    "            # Connect scan button to function with LLM interpretation\n",
    "            def scan_and_interpret(file_path):\n",
    "                \"\"\"Scan file and get LLM interpretation of results\"\"\"\n",
    "                if not file_path:\n",
    "                    return \"‚ùå Please select a file\", \"No file selected\", {}\n",
    "                \n",
    "                # Get scan results\n",
    "                markdown_result, json_result = scan_file_with_virustotal(file_path)\n",
    "                \n",
    "                # Get LLM interpretation of the results\n",
    "                if json_result and json_result.get(\"status\") == \"success\":\n",
    "                    llm_interpretation = interpret_scan_results_with_llm(json_result)\n",
    "                    ai_output = f\"## ü§ñ AI Analysis\\n\\n{llm_interpretation}\\n\\n---\\n\\n## üìã Technical Details\\n\\n{markdown_result}\"\n",
    "                else:\n",
    "                    ai_output = markdown_result\n",
    "                \n",
    "                return ai_output, markdown_result, json_result\n",
    "            \n",
    "            scan_button.click(\n",
    "                fn=scan_and_interpret,\n",
    "                inputs=file_input,\n",
    "                outputs=[ai_interpretation, scan_output, json_output]\n",
    "            )\n",
    "\n",
    "        # ===== TAB 3: AI Risk Hybrid Search =====\n",
    "        with gr.Tab(\"üß† AI Risk Search\"):\n",
    "            gr.Markdown(\"### AI Risk Database Hybrid Search\")\n",
    "            gr.Markdown(\"Lexical pre-filtering + semantic re-ranking over AI_risk_database_v4.csv\")\n",
    "\n",
    "            risk_query = gr.Textbox(\n",
    "                label=\"Ask a question\",\n",
    "                placeholder=\"e.g., What risks involve privacy leakage?\"\n",
    "            )\n",
    "            risk_top_k = gr.Slider(\n",
    "                minimum=3,\n",
    "                maximum=10,\n",
    "                value=5,\n",
    "                step=1,\n",
    "                label=\"Top results\"\n",
    "            )\n",
    "            risk_search_btn = gr.Button(\"üîé Search\", variant=\"primary\")\n",
    "\n",
    "            risk_results_md = gr.Markdown(\"Enter a question and click Search to see results.\")\n",
    "            risk_results_json = gr.JSON(label=\"üìÑ Raw Results (JSON)\", visible=True)\n",
    "\n",
    "            risk_search_btn.click(\n",
    "                fn=run_ai_risk_search,\n",
    "                inputs=[risk_query, risk_top_k],\n",
    "                outputs=[risk_results_md, risk_results_json]\n",
    "            )\n",
    "\n",
    "    landing_search_btn.click(\n",
    "        fn=landing_ai_risk_search,\n",
    "        inputs=[landing_query, landing_top_k],\n",
    "        outputs=[landing_output, risk_query, risk_results_md, risk_results_json]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Chatbot interface with LLM-powered file scanner created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc29b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VirusTotal API configured successfully\n",
      "   API Key: ea827edd89776ebcbbcf...\n",
      "   Base URL: https://www.virustotal.com/api/v3\n",
      "‚úÖ AI Risk Database hybrid search service initialized\n",
      "‚úÖ VirusTotal API Key loaded: ea8...\n",
      "‚úÖ VirusTotal scanner imported from service_1.py\n",
      "‚úÖ AI Risk hybrid search service imported from service_2.py\n",
      "‚úÖ Enterprise pattern: UI layer imports from service layer\n",
      "‚úÖ LLM-powered scan result interpreter loaded\n"
     ]
    }
   ],
   "source": [
    "## VirusTotal File Scanner Integration\n",
    "# Import from service_1.py (single source of truth - enterprise pattern)\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/retlawair/Desktop/deploying-ai/05_src/assignment-2')\n",
    "\n",
    "from service_1 import scan_file_with_details, VIRUSTOTAL_API_KEY\n",
    "from service_2 import hybrid_search, format_search_results_markdown\n",
    "import json\n",
    "\n",
    "# Alias to match Gradio naming convention\n",
    "scan_file_with_virustotal = scan_file_with_details\n",
    "\n",
    "print(f\"‚úÖ VirusTotal API Key loaded: {VIRUSTOTAL_API_KEY[:3]}...\")\n",
    "print(\"‚úÖ VirusTotal scanner imported from service_1.py\")\n",
    "print(\"‚úÖ AI Risk hybrid search service imported from service_2.py\")\n",
    "print(\"‚úÖ Enterprise pattern: UI layer imports from service layer\")\n",
    "\n",
    "\n",
    "def interpret_scan_results_with_llm(scan_json: dict) -> str:\n",
    "    \"\"\"\n",
    "    Use the LLM to interpret and explain VirusTotal scan results to the user.\n",
    "    \n",
    "    Args:\n",
    "        scan_json: The JSON result from VirusTotal scan\n",
    "        \n",
    "    Returns:\n",
    "        LLM's natural language interpretation of the scan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate and safely extract scan data\n",
    "        if not isinstance(scan_json, dict):\n",
    "            return f\"‚ö†Ô∏è Invalid scan data type: {type(scan_json).__name__}\"\n",
    "        \n",
    "        # Safely extract nested dicts and convert all values to strings\n",
    "        file_info = scan_json.get('file_info', {})\n",
    "        if not isinstance(file_info, dict):\n",
    "            file_info = {}\n",
    "        \n",
    "        scan_info = scan_json.get('scan_info', {})\n",
    "        if not isinstance(scan_info, dict):\n",
    "            scan_info = {}\n",
    "        \n",
    "        # Get individual values, converting to string to prevent type mismatches\n",
    "        try:\n",
    "            file_name = str(file_info.get('name', 'Unknown')).strip() or 'Unknown'\n",
    "            file_size = str(file_info.get('size_bytes', 'Unknown')).strip() or 'Unknown'\n",
    "            file_hash = str(file_info.get('sha256_hash', 'Unknown')).strip() or 'Unknown'\n",
    "            scan_status = str(scan_info.get('status', 'Unknown')).strip() or 'Unknown'\n",
    "            scan_id = str(scan_info.get('scan_id', 'Unknown')).strip() or 'Unknown'\n",
    "        except (ValueError, TypeError) as e:\n",
    "            return f\"‚ö†Ô∏è Error processing scan data: Could not parse file information\"\n",
    "        \n",
    "        # Format scan data for LLM analysis\n",
    "        try:\n",
    "            scan_summary = f\"\"\"\n",
    "**File Scan Summary:**\n",
    "- File: {file_name}\n",
    "- Size: {file_size} bytes\n",
    "- SHA256: {file_hash}\n",
    "- Scan Status: {scan_status}\n",
    "- Scan ID: {scan_id}\n",
    "\n",
    "Please provide a clear, concise interpretation of this file scan result for a non-technical user. \n",
    "Explain what the status means, what they should expect, and any recommended actions.\n",
    "\"\"\"\n",
    "        except (TypeError, ValueError) as e:\n",
    "            return \"‚ö†Ô∏è Error formatting scan summary for analysis\"\n",
    "        \n",
    "        # Call LLM to interpret results\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a cybersecurity expert explaining file scan results in simple, non-technical language. Be clear, concise, and actionable.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": scan_summary\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            interpretation = str(response.choices[0].message.content).strip()\n",
    "        except Exception as llm_error:\n",
    "            return f\"‚ö†Ô∏è Could not get LLM interpretation: {type(llm_error).__name__}\"\n",
    "        \n",
    "        # Apply guardrails to LLM output\n",
    "        try:\n",
    "            is_valid, result = guardrails.validate_output(interpretation)\n",
    "            return result\n",
    "        except GuardrailViolation:\n",
    "            return interpretation\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Safely convert exception to string\n",
    "        try:\n",
    "            error_msg = str(e)\n",
    "        except:\n",
    "            error_msg = type(e).__name__\n",
    "        return f\"‚ö†Ô∏è Could not interpret results: {error_msg}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ LLM-powered scan result interpreter loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd250b",
   "metadata": {},
   "source": [
    "## Step 7: Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddabfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                   üöÄ LAUNCHING CHATBOT APPLICATION                    \n",
      "======================================================================\n",
      "\n",
      "üì± Open your browser and navigate to: http://localhost:7860\n",
      "\n",
      "‚ú® Features:\n",
      "   ‚Ä¢ Full conversation history\n",
      "   ‚Ä¢ Real-time responses from GPT-4\n",
      "   ‚Ä¢ Amazing, responsive interface\n",
      "   ‚Ä¢ One-click chat management\n",
      "\n",
      "‚èπÔ∏è  Press Ctrl+C in terminal to stop\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Scanning file: eicar.com.txt\n",
      "   Size: 68 bytes\n",
      "   SHA256: 275a021bbfb6489e54d471899f7db9d1663fc695ec2fe2a2c4538aabf651fd0f\n",
      "‚úÖ File scan initiated successfully: NDRkODg2MTJmZWE4YThmMzZkZTgyZTEyNzhhYmIwMmY6MTc3MjE2NjgxNg==\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio app\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ LAUNCHING CHATBOT APPLICATION\".center(70))\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"üì± Open your browser and navigate to: http://localhost:7860\")\n",
    "print()\n",
    "print(\"‚ú® Features:\")\n",
    "print(\"   ‚Ä¢ Full conversation history\")\n",
    "print(\"   ‚Ä¢ Real-time responses from GPT-4\")\n",
    "print(\"   ‚Ä¢ Amazing, responsive interface\")\n",
    "print(\"   ‚Ä¢ One-click chat management\")\n",
    "print()\n",
    "print(\"‚èπÔ∏è  Press Ctrl+C in terminal to stop\")\n",
    "print()\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Launch\n",
    "demo.launch(\n",
    "    share=False,\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=None,\n",
    "    show_error=True,\n",
    "    quiet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8bdc16",
   "metadata": {},
   "source": [
    "## üìã Enterprise Guardrails System - Technical Documentation\n",
    "\n",
    "### Overview\n",
    "This chatbot implements **three-layer enterprise-grade guardrails** to ensure safe and compliant AI interactions:\n",
    "\n",
    "### 1Ô∏è‚É£ **Input Validation (Hard Block)**\n",
    "- **Type Checking**: Validates input is non-empty string\n",
    "- **Length Limits**: 1-2000 characters (configurable)\n",
    "- **Forbidden Topics**: Blocks requests mentioning:\n",
    "  - Cats, Dogs, Horoscopes, Zodiac Signs, Taylor Swift\n",
    "- **Pattern Detection**: Regex-based detection of:\n",
    "  - Prompt injection attempts\n",
    "  - System prompt exposure attempts\n",
    "  - Instruction override attempts\n",
    "- **Rate Limiting**: 60 requests/minute per user (hard limit)\n",
    "\n",
    "### 2Ô∏è‚É£ **Output Validation (Hard Block)**\n",
    "- **Type Safety**: Ensures response is valid string\n",
    "- **Length Enforcement**: 1-4000 characters with auto-truncation\n",
    "- **Content Filtering**: Detects forbidden topics in model output\n",
    "- **System Prompt Leakage Detection**: Prevents accidental exposure of system instructions\n",
    "- **Automatic Sanitization**: Truncates excessive outputs with \"...\" indicator\n",
    "\n",
    "### 3Ô∏è‚É£ **Hard Limits (Parameter Validation)**\n",
    "- **Temperature**: 0.0-2.0 (deterministic to highly creative)\n",
    "- **Max Tokens**: 100-4000 per request\n",
    "- **Input Length**: Maximum 2000 characters\n",
    "- **Output Length**: Maximum 4000 characters\n",
    "- **Request Rate**: 60 requests per minute per user\n",
    "\n",
    "### Architecture Benefits\n",
    "‚úÖ **Scalable**: Dataclass-based configuration for easy deployment variations  \n",
    "‚úÖ **Enterprise**: Severity levels (CRITICAL/WARNING) and violation tracking  \n",
    "‚úÖ **Maintainable**: Centralized GuardrailConfig class for policy updates  \n",
    "‚úÖ **Auditable**: Violation exceptions with detailed type and reason  \n",
    "‚úÖ **Safe Defaults**: Conservative limits that can be relaxed if needed  \n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "```python\n",
    "# Check guardrail status\n",
    "guardrails.get_guardrail_status()\n",
    "\n",
    "# Modify forbidden topics (if needed)\n",
    "guardrails.config.forbidden_topics.append(\"new_topic\")\n",
    "\n",
    "# Adjust rate limits\n",
    "guardrails.config.max_requests_per_minute = 120\n",
    "```\n",
    "\n",
    "### Violation Handling\n",
    "- **CRITICAL violations**: Request/response blocked, user receives ‚õî message\n",
    "- **WARNING violations**: Logged but execution may continue\n",
    "- All violations raise `GuardrailViolation` exception with severity/type info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
